/**
 * BullMQ Worker - Kuyruktan job alƒ±p Solana havuz analizlerini i≈üler
 * 
 * Bu worker:
 * - Redis kuyruƒüundan analiz isteklerini alƒ±r
 * - Helius API'den blockchain verileri √ßeker
 * - Daydreams/Anthropic Claude ile analiz yapar
 * - Sonu√ßlarƒ± Supabase'e kaydeder ve Redis'e cache'ler
 */

import 'dotenv/config';
import { Worker, Job } from 'bullmq';
import type { QueueJobData } from './lib/types';
import { heliusClient } from './lib/helius-client';
import { buildAnalysisPrompt, validateAnalysisResponse, parseRiskScore } from './lib/claude-prompt';
import { saveAnalysis } from './lib/supabase';
import { setCachedAnalysis } from './lib/cache';
import { redis } from './lib/cache'; // Redis connection'ƒ± payla≈üƒ±yoruz

// Environment validation
const REQUIRED_ENV = [
  'HELIUS_API_KEY',
  'INFERENCE_API_KEY',
  'REDIS_URL',
  'SUPABASE_URL',
  'SUPABASE_SERVICE_KEY',
];

for (const key of REQUIRED_ENV) {
  if (!process.env[key]) {
    console.error(`‚ùå Missing required environment variable: ${key}`);
    process.exit(1);
  }
}

// Configuration
const WORKER_CONFIG = {
  concurrency: parseInt(process.env.WORKER_CONCURRENCY || '5', 10),
  maxJobsPerWorker: 100,
  lockDuration: 120000, // 2 dakika - uzun analizler i√ßin
  lockRenewTime: 60000, // 1 dakikada bir lock yenile
};

// Daydreams API Configuration (using fetch like yama-agent)
const INFERENCE_API_KEY = process.env.INFERENCE_API_KEY;
const DAYDREAMS_BASE_URL = process.env.DAYDREAMS_BASE_URL || 'https://api-beta.daydreams.systems/v1';

/**
 * Ana analiz fonksiyonu
 */
async function processAnalysis(job: Job<QueueJobData>) {
  const { poolId, userId, options } = job.data;
  
  console.log(`\nüîÑ [Job ${job.id}] Starting analysis for pool: ${poolId}`);
  console.log(`üë§ User: ${userId || 'anonymous'}`);
  
  // Progress tracking
  await job.updateProgress(10);
  
  // Increment daily analysis counter
  try {
    const { incrementDailyCount } = await import('./middleware/rate-limiter');
    const dailyCount = await incrementDailyCount();
    console.log(`üìä [Job ${job.id}] Daily analysis count: ${dailyCount}`);
  } catch (error: any) {
    console.warn(`[Job ${job.id}] Failed to increment daily count:`, error.message);
  }
  
  try {
    // 1. Pool bilgilerini √ßek
    console.log(`üì° [Job ${job.id}] Fetching pool account info...`);
    await heliusClient.getPoolAccountInfo(poolId); // Validate pool exists
    await job.updateProgress(20);
    
    // 2. Pool reserves √ßek (artƒ±k TVL ve metadata dahil!)
    console.log(`üîç [Job ${job.id}] Parsing pool reserves...`);
    const reserves = await heliusClient.getPoolReserves(poolId);
    await job.updateProgress(30);
    
    // 3. Token metadata (artƒ±k reserves'de var, ama validation i√ßin tekrar √ßekelim)
    console.log(`ü™ô [Job ${job.id}] Fetching token metadata...`);
    const [tokenA, tokenB] = await Promise.all([
      heliusClient.getTokenMetadata(reserves.tokenAMint),
      heliusClient.getTokenMetadata(reserves.tokenBMint),
    ]);
    await job.updateProgress(40);
    
    // 4. Transaction history √ßek
    console.log(`üìä [Job ${job.id}] Fetching transaction history...`);
    const txLimit = options?.transactionLimit || 2000; // Phase 3: Balanced for quality vs rate limits
    const transactions = await heliusClient.getTransactionHistory(poolId, txLimit);
    await job.updateProgress(60);
    
    // 4.5. PHASE 3: Historical trend analysis (7 days)
    console.log(`üìà [Job ${job.id}] PHASE 3: Analyzing historical trend...`);
    let poolHistory: any = undefined;
    try {
      const { getPoolHistoryTrend } = await import('./lib/pool-history');
      const { supabase } = await import('./lib/supabase');
      poolHistory = await getPoolHistoryTrend(supabase, poolId, 7);
      console.log(`üìà [Job ${job.id}] ‚úÖ Historical trend: ${poolHistory.tvl.trend} TVL, ${poolHistory.volume.trend} volume`);
    } catch (error: any) {
      console.warn(`[Job ${job.id}] ‚ö†Ô∏è Historical trend analysis failed: ${error.message}`);
      // Continue without historical data
    }
    await job.updateProgress(65);
    
    // 4.6. PHASE 3: Algorithmic risk scoring
    console.log(`üéØ [Job ${job.id}] PHASE 3: Calculating algorithmic risk score...`);
    let riskScoreBreakdown: any = undefined;
    try {
      const { calculateRiskScore } = await import('./lib/risk-scorer');
      riskScoreBreakdown = calculateRiskScore(reserves, tokenA, tokenB, transactions, poolHistory);
      console.log(`üéØ [Job ${job.id}] ‚úÖ Algorithmic risk: ${riskScoreBreakdown.totalScore}/100 (${riskScoreBreakdown.riskLevel})`);
    } catch (error: any) {
      console.warn(`[Job ${job.id}] ‚ö†Ô∏è Risk scoring failed: ${error.message}`);
      // Continue without risk breakdown
    }
    await job.updateProgress(70);
    
    // 5. Claude prompt olu≈ütur
    console.log(`ü§ñ [Job ${job.id}] Building AI analysis prompt...`);
    const prompt = buildAnalysisPrompt({
      poolId,
      tokenA,
      tokenB,
      reserves, // Now includes TVL, pool health, etc.
      transactions,
      poolHistory, // PHASE 3: Historical trend
    });
    
    // 6. Claude'a g√∂nder (Daydreams Inference API - using fetch like yama-agent)
    console.log(`üß† [Job ${job.id}] Sending to AI for analysis...`);
    const model = process.env.REPORT_MODEL || 'openai/gpt-4o';
    const maxTokens = parseInt(process.env.MAX_COMPLETION_TOKENS || '4096', 10);
    
    const payload = {
      model,
      temperature: 0.3,
      max_tokens: maxTokens,
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    };
    
    const response = await fetch(`${DAYDREAMS_BASE_URL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${INFERENCE_API_KEY}`,
      },
      body: JSON.stringify(payload),
    });
    
    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`${response.status} ${response.statusText}: ${errorText}`);
    }
    
    const result = await response.json() as {
      choices?: Array<{ message?: { content?: string } }>;
      usage?: { total_tokens?: number };
    };
    
    await job.updateProgress(80);
    
    // 7. Yanƒ±tƒ± parse et ve validate et
    console.log(`‚úÖ [Job ${job.id}] Parsing AI response...`);
    const rawResponse = result.choices?.[0]?.message?.content || '';
    
    // Validate the response
    const validation = validateAnalysisResponse(rawResponse);
    if (!validation.isValid) {
      console.warn(`[Job ${job.id}] Analysis missing sections:`, validation.missingSections);
    }
    
    // Parse risk score from response
    const riskScore = parseRiskScore(rawResponse);
    
    // Build complete AnalysisResult object
    const analysisResult = {
      poolId,
      tokenA,
      tokenB,
      reserves,
      transactions,
      riskAnalysis: rawResponse,
      riskScore,
      generatedAt: new Date().toISOString(),
      modelUsed: model,
      poolHistory, // PHASE 3: Historical trend
      riskScoreBreakdown, // PHASE 3: Algorithmic risk score
    };
    
    // 8. Supabase'e kaydet
    console.log(`üíæ [Job ${job.id}] Saving to Supabase...`);
    const savedRecord = await saveAnalysis(analysisResult, userId);
    
    if (!savedRecord) {
      throw new Error('Failed to save analysis to database');
    }
    
    // 9. Redis cache'e yaz
    console.log(`‚ö° [Job ${job.id}] Caching result...`);
    await setCachedAnalysis(poolId, analysisResult);
    
    await job.updateProgress(100);
    
    console.log(`‚úÖ [Job ${job.id}] Analysis completed successfully!`);
    console.log(`üìÑ Record ID: ${savedRecord.id}`);
    console.log(`‚ö†Ô∏è  Risk Score: ${analysisResult.riskScore}/100`);
    
    // Helper function to serialize BigInt values
    const serializeBigInt = (obj: any): any => {
      if (obj === null || obj === undefined) return obj;
      if (typeof obj === 'bigint') return obj.toString();
      if (Array.isArray(obj)) return obj.map(item => serializeBigInt(item));
      if (typeof obj === 'object') {
        const serialized: any = {};
        for (const key in obj) {
          if (obj.hasOwnProperty(key)) {
            serialized[key] = serializeBigInt(obj[key]);
          }
        }
        return serialized;
      }
      return obj;
    };
    
    // Job sonucu - BigInt'leri serialize et
    return {
      success: true,
      recordId: savedRecord.id,
      poolId,
      riskScore: analysisResult.riskScore,
      analysisResult: serializeBigInt(analysisResult),
    };
    
  } catch (error: any) {
    console.error(`‚ùå [Job ${job.id}] Analysis failed:`, error.message);
    
    // Detaylƒ± hata logging
    if (error.response) {
      console.error('API Error Response:', {
        status: error.response.status,
        data: error.response.data,
      });
    }
    
    throw error; // BullMQ retry mekanizmasƒ± devreye girecek
  }
}

/**
 * Worker instance olu≈ütur
 */
const worker = new Worker<QueueJobData>(
  'pool-analysis', // Queue ismi (queue.ts ile aynƒ± olmalƒ±)
  processAnalysis,
  {
    connection: redis, // Redis baƒülantƒ±sƒ±nƒ± payla≈ü
    concurrency: WORKER_CONFIG.concurrency,
    lockDuration: WORKER_CONFIG.lockDuration,
    lockRenewTime: WORKER_CONFIG.lockRenewTime,
    maxStalledCount: 2, // 2 kez stall olursa job fail
    stalledInterval: 30000, // 30 saniyede bir stalled job kontrol√º
  }
);

/**
 * Event Listeners
 */

worker.on('ready', () => {
  console.log('üöÄ Worker is ready and waiting for jobs...');
  console.log(`‚öôÔ∏è  Concurrency: ${WORKER_CONFIG.concurrency}`);
  console.log(`üîí Lock Duration: ${WORKER_CONFIG.lockDuration / 1000}s`);
  console.log(`ü§ñ Model: ${process.env.REPORT_MODEL || 'claude-3-5-sonnet-20241022'}`);
  console.log(`üìä Transaction Limit: ${process.env.TRANSACTION_LIMIT || 2000}`);
});

worker.on('active', (job) => {
  console.log(`\n‚ñ∂Ô∏è  [Job ${job.id}] Started processing...`);
});

worker.on('completed', (job, result) => {
  console.log(`\n‚úÖ [Job ${job.id}] Completed in ${Date.now() - job.timestamp}ms`);
  console.log(`   Pool: ${result.poolId}`);
  console.log(`   Risk Score: ${result.riskScore}/100`);
});

worker.on('failed', (job, err) => {
  if (job) {
    console.error(`\n‚ùå [Job ${job.id}] Failed after ${job.attemptsMade} attempts`);
    console.error(`   Error: ${err.message}`);
    console.error(`   Pool: ${job.data.poolId}`);
  } else {
    console.error(`\n‚ùå Job failed: ${err.message}`);
  }
});

worker.on('progress', (job, progress) => {
  console.log(`   üìà [Job ${job.id}] Progress: ${progress}%`);
});

worker.on('stalled', (jobId) => {
  console.warn(`‚ö†Ô∏è  [Job ${jobId}] Stalled! Retrying...`);
});

worker.on('error', (err) => {
  console.error('üí• Worker error:', err);
});

/**
 * Graceful Shutdown
 */
const shutdown = async (signal: string) => {
  console.log(`\n\nüõë Received ${signal}, shutting down gracefully...`);
  
  try {
    // Worker'ƒ± durdur (aktif job'larƒ± bitirmesine izin ver)
    await worker.close();
    console.log('‚úÖ Worker closed');
    
    // Redis baƒülantƒ±sƒ±nƒ± kapat
    await redis.quit();
    console.log('‚úÖ Redis connection closed');
    
    console.log('üëã Goodbye!');
    process.exit(0);
  } catch (error: any) {
    console.error('‚ùå Error during shutdown:', error.message);
    process.exit(1);
  }
};

// Signal handlers
process.on('SIGINT', () => shutdown('SIGINT'));
process.on('SIGTERM', () => shutdown('SIGTERM'));

// Unhandled rejection handler
process.on('unhandledRejection', (reason, promise) => {
  console.error('üí• Unhandled Rejection at:', promise);
  console.error('üí• Reason:', reason);
  // Production'da process.exit(1) yapƒ±labilir
});

console.log('üéØ Solana Liquidity Analysis Worker');
console.log('====================================');
console.log(`üìÖ Started at: ${new Date().toISOString()}`);
console.log(`üîß Node/Bun Version: ${process.version}`);
console.log(`üíª Platform: ${process.platform}`);
console.log('');

